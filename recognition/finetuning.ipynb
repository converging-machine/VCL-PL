{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Rc5E-MOY12I5"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torch.utils.data as data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torchsummary import summary\n",
    "import config\n",
    "from facenet_pytorch import training\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision import datasets, transforms\n",
    "from PIL import Image\n",
    "import glob\n",
    "import torchvision.models as models\n",
    "from util import AverageMeter, learning_rate_decay, Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Root directory for dataset\n",
    "data_root = \"/home/mehmetyavuz/datasets/CelebA128/\"\n",
    "attr_root = \"/home/mehmetyavuz/datasets/list_attr_celeba.txt\"\n",
    "# Number of workers for dataloader\n",
    "workers = 4\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 64\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = (128,128)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CelebA(data.Dataset):\n",
    "    def __init__(self, data_path, attr_path, image_size, mode, selected_attrs):\n",
    "        super(CelebA, self).__init__()\n",
    "        self.data_path = data_path\n",
    "        att_list = open(attr_path, 'r', encoding='utf-8').readlines()[1].split()\n",
    "        atts = [att_list.index(att) + 1 for att in selected_attrs]\n",
    "        images = np.loadtxt(attr_path, skiprows=2, usecols=[0], dtype=np.str)\n",
    "        labels = np.loadtxt(attr_path, skiprows=2, usecols=atts, dtype=np.int)\n",
    "        \n",
    "        self.tf = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        self.tf_a = transforms.Compose([\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([\n",
    "                transforms.ColorJitter(hue=.05, saturation=.05),\n",
    "            ], p=0.8),\n",
    "            transforms.RandomGrayscale(0.2),\n",
    "        ])        \n",
    "        if mode == 'train':\n",
    "            self.images = images[:162770]\n",
    "            self.labels = labels[:162770]\n",
    "\n",
    "        if mode == 'valid':\n",
    "            self.images = images[162770:182637]\n",
    "            self.labels = labels[162770:182637]\n",
    "\n",
    "        if mode == 'test':\n",
    "            self.images = images[182637:]\n",
    "            self.labels = labels[182637:]\n",
    "                                       \n",
    "        self.length = len(self.images)\n",
    "    def __getitem__(self, index):\n",
    "        if index < 162770:\n",
    "            img = self.tf(self.tf_a(Image.open(os.path.join(self.data_path, self.images[index]))))\n",
    "        else:\n",
    "            img = self.tf(Image.open(os.path.join(self.data_path, self.images[index])))\n",
    "        att = torch.tensor((self.labels[index] + 1) // 2)\n",
    "        return img, att.to(torch.float32)\n",
    "    def __len__(self):\n",
    "        return self.length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_default = [\"5_o_Clock_Shadow\", \"Arched_Eyebrows\", \"Attractive\", \"Bags_Under_Eyes\", \"Bald\", \"Bangs\", \"Big_Lips\", \"Big_Nose\", \n",
    "                 \"Black_Hair\", \"Blond_Hair\", \"Blurry\", \"Brown_Hair\", \"Bushy_Eyebrows\", \"Chubby\", \"Double_Chin\", \"Eyeglasses\", \"Goatee\", \n",
    "                 \"Gray_Hair\", \"Heavy_Makeup\", \"High_Cheekbones\", \"Male\", \"Mouth_Slightly_Open\", \"Mustache\", \"Narrow_Eyes\", \"No_Beard\", \n",
    "                 \"Oval_Face\", \"Pale_Skin\", \"Pointy_Nose\", \"Receding_Hairline\", \"Rosy_Cheeks\", \"Sideburns\", \"Smiling\", \"Straight_Hair\", \n",
    "                 \"Wavy_Hair\", \"Wearing_Earrings\", \"Wearing_Hat\", \"Wearing_Lipstick\", \"Wearing_Necklace\", \"Wearing_Necktie\", \"Young\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CelebA(data_root, attr_root, image_size, 'train', attrs_default)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=True,\n",
    "                                          num_workers=workers)\n",
    "dataset = CelebA(data_root, attr_root, image_size, 'valid', attrs_default)\n",
    "val_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=workers)\n",
    "dataset = CelebA(data_root, attr_root, image_size, 'test', attrs_default)\n",
    "test_loader = torch.utils.data.DataLoader(dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet = models.alexnet(pretrained=True).to(device)\n",
    "resnet.classifier[6] = nn.Linear(in_features=4096, out_features=40, bias=True).to(device)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "weights = torch.load(\"alexnet*.pth\")\n",
    "resnet.load_state_dict(weights,strict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AlexNet(\n",
       "  (features): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(11, 11), stride=(4, 4), padding=(2, 2))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (3): Conv2d(64, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "    (4): ReLU(inplace=True)\n",
       "    (5): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (6): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (7): ReLU(inplace=True)\n",
       "    (8): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (9): ReLU(inplace=True)\n",
       "    (10): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(6, 6))\n",
       "  (classifier): Sequential(\n",
       "    (0): Dropout(p=0.5, inplace=False)\n",
       "    (1): Linear(in_features=9216, out_features=4096, bias=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): Dropout(p=0.5, inplace=False)\n",
       "    (4): Linear(in_features=4096, out_features=4096, bias=True)\n",
       "    (5): ReLU(inplace=True)\n",
       "    (6): Linear(in_features=4096, out_features=40, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 0\n",
    "for i, (name, param) in enumerate(resnet.named_parameters()):\n",
    "    j = i // 2\n",
    "    if epoch % 2 == 0:\n",
    "        if j % 2 == 0:\n",
    "            if 'weight' in name:\n",
    "                param.requires_grad = False\n",
    "            if 'bias' in name:\n",
    "                param.requires_grad = False\n",
    "        else:\n",
    "            if 'weight' in name:\n",
    "                param.requires_grad = True\n",
    "            if 'bias' in name:\n",
    "                param.requires_grad = True       \n",
    "    else:\n",
    "        if j % 2 == 0:        \n",
    "            if 'weight' in name:\n",
    "                param.requires_grad = True\n",
    "            if 'bias' in name:\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            if 'weight' in name:\n",
    "                param.requires_grad = False\n",
    "            if 'bias' in name:\n",
    "                param.requires_grad = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features.3.weight\n",
      "features.3.bias\n",
      "features.8.weight\n",
      "features.8.bias\n",
      "classifier.1.weight\n",
      "classifier.1.bias\n",
      "classifier.6.weight\n",
      "classifier.6.bias\n"
     ]
    }
   ],
   "source": [
    "for i, (name, param) in enumerate(resnet.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (name, param) in enumerate(resnet.named_parameters()):\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(resnet.parameters(), lr=0.00001)\n",
    "#Q = math.floor(len(train_dataset)*epochs)\n",
    "scheduler = None #torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "metrics = {\n",
    "    'acc': training.accuracy_ml\n",
    "} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n\\nInitial')\n",
    "print('-' * 10)\n",
    "resnet.eval()\n",
    "training.pass_epoch(\n",
    "    resnet, loss_fn, test_loader,\n",
    "    batch_metrics=metrics, show_running=True, device=device,\n",
    "    #writer=writer\n",
    ")\n",
    "\n",
    "val_loss = 1\n",
    "for epoch in range(epochs):\n",
    "    print('\\nEpoch {}/{}'.format(epoch + 1, epochs))\n",
    "    print('-' * 10)\n",
    "\n",
    "    resnet.train()\n",
    "    training.pass_epoch(\n",
    "        resnet, loss_fn, train_loader, optimizer, scheduler,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        #writer=writer\n",
    "    )\n",
    "\n",
    "    resnet.eval()\n",
    "    val_metrics = training.pass_epoch(\n",
    "        resnet, loss_fn, val_loader,\n",
    "        batch_metrics=metrics, show_running=True, device=device,\n",
    "        #writer=writer\n",
    "    )\n",
    "    \n",
    "    if val_metrics[0].item() < val_loss:\n",
    "        val_loss = val_metrics[0].item()\n",
    "        print('Test set Accuracy Lowest Validation Loss:')\n",
    "        training.pass_epoch(\n",
    "            resnet, loss_fn, test_loader,\n",
    "            batch_metrics=metrics, show_running=True, device=device,\n",
    "            #writer=writer\n",
    "        )\n",
    "        torch.save(resnet.state_dict(), \"alexnet.pth\")\n",
    "\n",
    "#writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "AE.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
